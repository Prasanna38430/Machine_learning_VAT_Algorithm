{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visual Assessment of Tendency (VAT) - Introduction**\n",
    "\n",
    "The **VAT algorithm** helps us **see** how many groups (clusters) exist in a dataset before applying clustering methods like K-Means.\n",
    "\n",
    "\n",
    "\n",
    "### How It Works\n",
    "1. **Measure Similarity**  \n",
    "   - VAT calculates how similar or different each data point is from the others.  \n",
    "   - This is done using distances (e.g., Euclidean distance).  \n",
    "\n",
    "2. **Reorder the Data**  \n",
    "   - VAT rearranges the data to place similar points closer together.  \n",
    "   - This makes it easier to spot groups in the data.  \n",
    "\n",
    "3. **Create a Visual Map**  \n",
    "   - VAT makes an image (heatmap) of the reordered data.  \n",
    "   - Dark blocks in the image show clusters (groups of similar points).  \n",
    "\n",
    "### How to Read the VAT Image\n",
    "- **Dark squares along the diagonal** → Show clusters (groups).  \n",
    "- **Bright lines between squares** → Show gaps between clusters.  \n",
    "- **More dark squares** → More clusters in the data.  \n",
    "\n",
    "### Why Use VAT?\n",
    "- **Easy to understand** – It gives a clear picture of data structure.  \n",
    "- **No need to set cluster numbers** – Helps decide how many groups exist.  \n",
    "- **Works for any type of data** – Can be used for numbers and categories.  \n",
    "\n",
    "VAT is useful when you don’t know how many clusters are in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_circles, load_iris\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import warnings\n",
    "\n",
    "class VAT:\n",
    "    \n",
    "    def __init__(self, normalize=True, colormap='gray_r', n_samples_max=5000):\n",
    "        self.normalize = normalize\n",
    "        self.n_samples_max = n_samples_max\n",
    "        self.cmap = plt.cm.gray_r if colormap == 'gray_r' else LinearSegmentedColormap.from_list('vat_cmap', ['black', 'white'], N=256)\n",
    "\n",
    "    def fit(self, data):\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = self.preprocess_data(data)   \n",
    "        if len(data) > self.n_samples_max:\n",
    "            data = data[np.random.choice(len(data), self.n_samples_max, replace=False)] \n",
    "        self.original_data = data.copy()    \n",
    "        if data.shape[1] > 2 and self.is_nonlinear(data):\n",
    "            self.R_ = self.geodesic_distance(data)\n",
    "        else:\n",
    "            self.R_ = squareform(pdist(data, 'euclidean'))   \n",
    "        if self.normalize:\n",
    "            self.R_ = (self.R_ - self.R_.min()) / (self.R_.max() - self.R_.min()) \n",
    "        \n",
    "        self.order_ = self.vat_ordering(self.R_)\n",
    "        self.R_ordered_ = self.R_[np.ix_(self.order_, self.order_)]\n",
    "        return self\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        num_cols = data.select_dtypes(include=[np.number]).columns\n",
    "        cat_cols = data.select_dtypes(exclude=[np.number]).columns\n",
    "        \n",
    "        data[num_cols] = SimpleImputer(strategy='mean').fit_transform(data[num_cols])\n",
    "        data[cat_cols] = SimpleImputer(strategy='most_frequent').fit_transform(data[cat_cols])   \n",
    "      \n",
    "        if len(cat_cols) > 0:\n",
    "            encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "            encoded = encoder.fit_transform(data[cat_cols])\n",
    "            return np.hstack((data[num_cols].values, encoded))\n",
    "        return data[num_cols].values\n",
    "    \n",
    "    def is_nonlinear(self, X):\n",
    "        pca = PCA(n_components=2).fit(X)\n",
    "        return pca.explained_variance_ratio_[0] < 0.6\n",
    "    \n",
    "    def geodesic_distance(self, X):\n",
    "        n_neighbors = min(15, X.shape[0]-1)\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(X)\n",
    "        distances, indices = nbrs.kneighbors(X) \n",
    "        dist_matrix = np.zeros((X.shape[0], X.shape[0]))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j, d in zip(indices[i], distances[i]):\n",
    "                if i != j:\n",
    "                    dist_matrix[i,j] = d\n",
    "                    dist_matrix[j,i] = d\n",
    "        dist_matrix[dist_matrix == 0] = np.inf\n",
    "        np.fill_diagonal(dist_matrix, 0)\n",
    "        return shortest_path(dist_matrix, directed=False)\n",
    "    \n",
    "    def vat_ordering(self, R):\n",
    "        n = R.shape[0]\n",
    "        P = np.zeros(n, dtype=int)\n",
    "        J = set(range(n))  \n",
    "        max_idx = np.unravel_index(np.argmax(R), R.shape)\n",
    "        P[0] = max_idx[0]\n",
    "        I = {P[0]}\n",
    "        J.remove(P[0])      \n",
    "        for r in range(1, n):\n",
    "            submatrix = R[np.array(list(I))[:, None], np.array(list(J))]\n",
    "            min_val = np.min(submatrix)\n",
    "            min_mask = submatrix == min_val       \n",
    "            candidates = np.argwhere(min_mask)\n",
    "            if len(candidates) == 0:\n",
    "                j_in_J = list(J)[0]\n",
    "                P[r] = j_in_J\n",
    "                I.add(j_in_J)\n",
    "                J.remove(j_in_J)\n",
    "                continue              \n",
    "            selected = candidates[np.argmax(candidates[:, 0])]\n",
    "            i_in_I = list(I)[selected[0]]\n",
    "            j_in_J = list(J)[selected[1]]\n",
    "            \n",
    "            P[r] = j_in_J\n",
    "            I.add(j_in_J)\n",
    "            J.remove(j_in_J)      \n",
    "        return P\n",
    "    \n",
    "    \n",
    "    \n",
    "    def plot_paper_style(self, title=None, figsize=(10, 5)):\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)  \n",
    "       \n",
    "        img1 = ax1.imshow(self.R_, cmap=self.cmap, aspect='equal',\n",
    "                         vmin=0, vmax=1 if self.normalize else None)\n",
    "        ax1.set_title('Random Dissimilarity Matrix', pad=15)\n",
    "        ax1.axis('off')\n",
    "        plt.colorbar(img1, ax=ax1, shrink=0.7)  \n",
    "       \n",
    "        img2 = ax2.imshow(self.R_ordered_, cmap=self.cmap, aspect='equal',\n",
    "                         vmin=0, vmax=1 if self.normalize else None)\n",
    "        ax2.set_title('Ordered Dissimilarity Matrix', pad=15)\n",
    "        ax2.axis('off')\n",
    "        plt.colorbar(img2, ax=ax2, shrink=0.7)  \n",
    "        if title:\n",
    "            fig.suptitle(title, y=1.02)   \n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
